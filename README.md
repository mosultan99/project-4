
"""text mining and sentimental analysis of user reviews

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1grQbmxtP7fMDzXxoTpbooC0VJooUzQg7

#part one text mining
preprocessing steps
"""

import sys
print(sys.executable)

!pip install wordcloud

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import re
from wordcloud import wordcloud
import nltk
nltk.download(['stopwords',
               'punkt',
               'wordnet',
               'omw-1.4',
               'vader_lexicon'
               ])

simple_text='This isn\'t a real text, this is an example text...Notice this contains punctuation!!'

tokenizer = nltk.tokenize.RegexpTokenizer('[a-zA-Z0-9\']+')
tokenized_document = tokenizer.tokenize(simple_text)
print(tokenized_document)

stop_words = nltk.corpus.stopwords.words('english')
print(stop_words)

#remove stopwords

cleaned_tokens = []

for word in tokenized_document:
  word = word.lower()
  if word not in stop_words:
    cleaned_tokens.append(word)
  print(cleaned_tokens)

# removing stopwords using list comprehension

cleaned_tokens = [word.lower() for word in tokenized_document if word.lower() not in stop_words]

print(cleaned_tokens)

# explore lemmatization vs stemming

lemmatizer = nltk.stem.WordNetLemmatizer()
stemmer = nltk.stem.PorterStemmer()

words = ['cacti','sings','hopped','rocks','better','easily']
pos = ['n','v','v','n','a','r']
lemmatized_words = [lemmatizer.lemmatize(words[i], pos=pos[i]) for i in range(6)]
stemmed_words = [stemmer.stem(word) for word in words]

print("lemmatized_words: ", lemmatized_words)
print("stemmed words: ", stemmed_words)

# now carrying out stemming on the example sentence

stemmed_text = [stemmer.stem(word) for word in cleaned_tokens]

print(stemmed_text)

# creating a function to apply all of the data preprocessing steps which can then be used on a corpus

def preprocess_text(text):
  tokenized_document = nltk.tokenize.RegexpTokenizer('[a-zA-Z0-9\']+').tokenize(text) #tokenize
  cleaned_tokens = [word.lower() for word in tokenized_document if word.lower() not in stop_words] #remove
  stemmed_text = [nltk.stem.PorterStemmer().stem(word) for word in cleaned_tokens] # stemming
  return stemmed_text

data = pd.read_csv("yelp_labelled.txt",sep="\t",header=None)
data.columns = ["Text", "Target_Label"]
data.head()

print("\n ALL Data Labels")
print(data.groupby("Target_Label").count())

print("\n ALL Data Labels")
print(data.groupby("Target_Label").count())

data['Text'] = data['Text'].apply(preprocess_text)

data.head()

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
X=vectorizer.fit_transform(data['Text'].map(''.join))
X=pd.DataFrame(X.toarray())
X.head()

from sklearn.model_selection import train_test_split

y = data['Target_Label']

X_train, X_test, y_train, y_test = train_test_split(
   X, y, train_size=0.8,test_size=0.2,random_state=99)

from imblearn.under_sampling import RandomUnderSampler

resampler = RandomUnderSampler(random_state=0)
X_train, y_train = resampler.fit_resample(X_train, y_train)

sns.countplot(x=y_train, color = 'blue', palette= ['tab:blue', 'tab:orange'])

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# computing the accuracy and making the confusion matrix
from sklearn import metrics
acc=metrics.accuracy_score(y_test,y_pred)
print('accuracy:%.2f\n\n'%(acc))
cm = metrics.confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm,'\n\n')
print('......................................................')
result = metrics.classification_report(y_test, y_pred)
print("Classification Report:\n",)
print (result)
